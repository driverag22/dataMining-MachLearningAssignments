{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1\n",
    "Diego Rivera Garrido(1674196), d.rivera.garrido@student.tue.nl\n",
    "\n",
    "Iliyan vasilev Teofilov(1671952), i.v.teofilov@student.tue.nl\n",
    "\n",
    "Nicolas Martinez van der Looven(2064839), n.martinez.van.der.looven@student.tue.nl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we implemented python functions forthe sigmoide, f and ∇f."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoide(x):\n",
    "    return np.exp(x)/(1+np.exp(x))\n",
    "\n",
    "def f(x):\n",
    "    w,b = x[0],x[1]\n",
    "    #return -np.log(sigmoide(w+b))-np.log(sigmoide(1.5*w+b))-np.log(sigmoide(-2*w-b))\n",
    "    return -0.5*w -b + np.log(1 + np.exp(w+b)) + np.log(1 + np.exp(1.5*w+b))+ np.log(1 + np.exp(-2*w-b))\n",
    "\n",
    "def grad_f(x):\n",
    "    w,b = x[0],x[1]\n",
    "    return np.array([-0.5 + sigmoide(w+b) + 1.5*sigmoide(1.5*w+b) - 2*sigmoide(-2*w-b),\n",
    "            -1 + sigmoide(w+b) + sigmoide(1.5*w+b) - sigmoide(-2*w-b)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we implemented the gradient descent algorithm given starting point w₀,b₀, f, ∇f and the stepsize function η(t). The result is the last iteration function value f(x) and the lowest (best) function value achieved throughout all iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f, grad_f, eta, w_0, b_0, max_iter=100):\n",
    "    x = np.array([w_0,b_0])\n",
    "    fmin = np.inf\n",
    "    for t in range(0,max_iter):\n",
    "        x = x - eta(t)*grad_f(x)\n",
    "        fx = f(x)\n",
    "        if (fx < fmin): \n",
    "            fmin = fx\n",
    "    return fx,fmin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.a\n",
    "Run algorithm with  constant stepsize strategy, η=0.2 constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eta_const(t,c=0.2):\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_descent(f,grad_f,eta_const,1,1,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.b\n",
    "Run algorithm with decreasing step-size strategy, η(t)=0.2/√(t+1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eta_sqrt(t,c=0.2):\n",
    "    return c/(np.sqrt(t+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_descent(f,grad_f,eta_sqrt,1,1,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.c\n",
    "Run algorithm with multi-step step-size strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eta_multistep(t, milestones=[20,50,80], c=0.2, eta_init=0.2):\n",
    "    factor = len(milestones)\n",
    "    for i in range(len(milestones)):\n",
    "        if t < milestones[i]:\n",
    "            factor = i\n",
    "            break\n",
    "    return eta_init * c**factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_descent(f,grad_f,eta_multistep,1,1,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Coordinate Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we implemented a function for evaluating f(x)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return (1/2) *(x[0] ** 4) - x[0] * x[1] + (x[1] ** 2) + x[1] * x[2] + (x[2] ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.a\n",
    "To compute argmin_xi f(x), we will compute the partial derivative of f wrt xi and equal it to zero to check the stationary points, in other words, we solve for xi in df(x)/dxi=0. Next, we find which stationary point evaluated on the second derivative wrt xi yields a positive value to find the minimum.\n",
    "\n",
    "For x₁, we solve df/dx₁ = 2x₁^3 - x₂ = 0 and d²f/dx₁² = 6x₁^2 > 0 -> x₁ = (x₂/2)^⅓ \n",
    "\n",
    "For x₂, we solve df/dx₂ = -x₁ + 2x₂ + x₃ = 0 and d²f/dx₂² = 2 > 0 -> x₂ = (x₁ - x₃)/2\n",
    "\n",
    "For x₃, we solve df/dx₃ = x₂ + 2x₃ = 0 and d²f/dx₃² = 2 > 0 -> x₃ = -x₂/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmin(x): # mind the index shift\n",
    "    # x₁ = (x₂/2)^⅓ \n",
    "    argminx0 = pow(x[1]/2, 1/3).real # truncate to real part\n",
    "    # x₂ = (x₁ - x₃)/2\n",
    "    argminx1 = (x[0]-x[2])/2\n",
    "    # x₃ = -x₂/2\n",
    "    argminx2 =  -x[1]/2\n",
    "    return (argminx0,argminx1,argminx2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_0=[2,3,4]\n",
    "for i in range(0,3):\n",
    "    print(\"i = \" + str(i) + \": \" + str(argmin(x_0)[i]))\n",
    "    print()\n",
    "\n",
    "\n",
    "## quick test\n",
    "for t in range(-100, 100):\n",
    "    if ( f( [t, x_0[1], x_0[2]] ) < f( [argmin(x_0)[0], x_0[1], x_0[2]] ) ):\n",
    "        print(\"not min.\")\n",
    "    if ( f( [x_0[0], t, x_0[2]] ) < f( [x_0[0], argmin(x_0)[1], x_0[2]] ) ):\n",
    "        print(\"not min.\")\n",
    "    if ( f( [x_0[0], x_0[1], t] ) < f( [x_0[0], x_0[1], argmin(x_0)[2]] ) ):\n",
    "        print(\"not min.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.b\n",
    "\n",
    "Next, we implement the coordinate descent algorithm given f, the array of argmin and the starting point x0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def coordinate_descent(f, argmin, x_0, max_iter=100):\n",
    "\n",
    "    print(\"t =\", 0, \":\")\n",
    "    print(\"x_t =\", x_0)\n",
    "    print(\"f(x_t) =\", f(x_0))\n",
    "    print()\n",
    "\n",
    "    x_t = x_0\n",
    "    for t in range(1, max_iter+1):\n",
    "        # use x_t to save values of argmin we find\n",
    "        print(\"t =\", t, \":\")\n",
    "        for i in range(0,3):\n",
    "            x_t[i] = argmin(x_t)[i] \n",
    "        print(\"x_t =\", x_t)\n",
    "        print(\"f(x_t) =\", f(x_t))\n",
    "        print()\n",
    "    return x_t, f(x_t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_0=[1,20,5]\n",
    "coordinate_descent(f, argmin, x_0, 100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Regression - polynomial features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.a\n",
    "\n",
    "In this exercise we will be using the California housing dataset from sklearn.\n",
    "\n",
    "We will compute the desing matrix for this dataset using a polynomial with degree 2, using the function PolynomialFeatures from sklearn.preprocessing. Because the data given has values in different big ranges, this function cannot compute the values for the matrix properly. To solve this what we do is standardize the data using the function StandardScaler from sklearn.preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import scipy as sp\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import PolynomialFeatures,StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the data matrix\n",
    "california = fetch_california_housing()\n",
    "D = california.data\n",
    "y = california.target\n",
    "n,d = D.shape\n",
    "print(n,d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a design matrix with polynomial standardized features\n",
    "aff = PolynomialFeatures(2,include_bias=True)\n",
    "scaler = StandardScaler()\n",
    "X = aff.fit_transform(scaler.fit_transform(D))\n",
    "features = aff.get_feature_names_out(california.feature_names)\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will compute the regression model minimizing the RSS for the polynomial\n",
    "design matrix. To minimize RSS, we solve the system of equations given by ∇(RSS)=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "beta = sp.linalg.solve(X.T@X,X.T@y)\n",
    "beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch beta values for given features\n",
    "print(beta[np.where(features == \"MedInc\")])\n",
    "print(beta[np.where(features == \"MedInc AveBedrms\")])\n",
    "print(beta[np.where(features == \"HouseAge AveBedrms\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.b\n",
    "\n",
    "In this subexercise we will repeat what we did in 3.a, in this case using ridge regression with the following objective: \\\n",
    "min f(β) = min 1/n*‖y-Xβ‖²+λ‖β‖²"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimize Objective function f(β)=1/n*‖y-Xβ‖²+λ‖β‖² by solving the system of equations \n",
    "# given by ∇(f(β))=0 for λ=0.1\n",
    "p = X.shape[1]\n",
    "beta_b = sp.linalg.solve(X.T@X+n*0.1*np.eye(p),X.T@y)\n",
    "beta_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch beta values for given features\n",
    "print(beta_b[np.where(features == \"MedInc\")])\n",
    "print(beta_b[np.where(features == \"MedInc AveBedrms\")])\n",
    "print(beta_b[np.where(features == \"HouseAge AveBedrms\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias-var trade \n",
    "\n",
    "In this exercise we will compute the bias² and variance of the given regression models: \n",
    "<ul>\n",
    "<li> True regression function: f*(x)=tan(πx)\n",
    "    <li> Fitted three regression models on the i.i.d. data samples D1,D2,D3 obtaining:\n",
    "​    <ul>\n",
    "        <li> f₁(x) = x + 0.2\n",
    "        <li> f₂(x) = 3x + 0.3\n",
    "        <li> f₃(x) = 5x + 0.1\n",
    "    <ul>    \n",
    "<ul>\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define functions to evaluate f*, f1, f2 and f3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fstar(x): return np.tan(np.pi*x)\n",
    "def fd1(x): return x + 0.2\n",
    "def fd2(x): return 3*x + 0.3\n",
    "def fd3(x): return 5*x + 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets plot the function to have and idea of how the regression functions fit the true function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "x = np.linspace(-0.4,0.4)\n",
    "\n",
    "plt.plot(x,fstar(x), label='f*')\n",
    "plt.plot(x,fd1(x), label='f1')\n",
    "plt.plot(x,fd2(x), label='f2')\n",
    "plt.plot(x,fd3(x), label='f3')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, lets compute for x₀=0 the bias² and the variance of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdx0 = np.array([fd1(0),fd2(0),fd3(0)])\n",
    "E = np.mean(fdx0)\n",
    "bias = (fstar(0)-E)\n",
    "var = np.mean((E-fdx0)**2)\n",
    "\n",
    "print(bias**2)\n",
    "print(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes - 20news\n",
    "From the 20Newsgroups dataset we fetch the documents belonging to three categories, which we use as classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "categories = ['alt.atheism', 'talk.politics.guns',\n",
    "              'sci.space']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = fetch_20newsgroups(subset='train', categories=categories)\n",
    "test = fetch_20newsgroups(subset='test', categories=categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, the first document in the training data is the following one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classes are indicated categorically with indices from zero to two by the target vector. The target names tell us which index belongs to which class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train.target\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We represent the documents in a bag of word format. That is, we create a data matrix ``D`` such that ``D[j,i]=1`` if the j-th document contains the i-th feature (word), and ``D[j,i]=0`` otherwise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(stop_words=\"english\", min_df=5,token_pattern=\"[^\\W\\d_]+\", binary=True)\n",
    "D = vectorizer.fit_transform(train.data)\n",
    "D_test = vectorizer.transform(test.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the allocation of feature indices to words by the following array, containing the vocabulary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, the word `naive` has the index 4044."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(vectorizer.get_feature_names_out() == 'naive')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.a\n",
    "First lets compute the class prior probabilities p(y):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_0 = np.array([x for x in y_train if x == 0])\n",
    "y_train_1 = np.array([x for x in y_train if x == 1])\n",
    "y_train_2 = np.array([x for x in y_train if x == 2])\n",
    "\n",
    "p_train_0 = y_train_0.size / y_train.size\n",
    "p_train_1 = y_train_1.size / y_train.size\n",
    "p_train_2 = y_train_2.size / y_train.size\n",
    "\n",
    "p_train_0, p_train_1, p_train_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.b\n",
    "Lets continue by computing the log-probabilities of the word 'naive' given each class, using Laplace smoothing with α=1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1e-5\n",
    "# Count number of documents of each class in the training set\n",
    "I_0 = np.where(y_train == 0)[0]\n",
    "I_1 = np.where(y_train == 1)[0]\n",
    "I_2 = np.where(y_train == 2)[0]\n",
    "\n",
    "# Count number of documents per class which includes the word 'naive'\n",
    "class_counts = {0: 0, 1: 0, 2: 0}\n",
    "for i in range(y_train.size):\n",
    "    if D[i, 4044] == 1:\n",
    "        class_counts[y_train[i]] += 1\n",
    "\n",
    "K = vectorizer.get_feature_names_out().size\n",
    "\n",
    "p_train_0 = (class_counts[0] + alpha) / (I_0.size + alpha * K)\n",
    "p_train_1 = (class_counts[1] + alpha) / (I_1.size + alpha * K)\n",
    "p_train_2 = (class_counts[2] + alpha) / (I_2.size + alpha * K)\n",
    "\n",
    "np.log(np.array([p_train_0, p_train_1, p_train_2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.c\n",
    "Finally, we will compute the class-conditioned log-probabilities for each word and class combination for the first document in the training dataset, using the naive Bayes algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1e-5\n",
    "# Count number of documents of each class in the training set\n",
    "I_0 = np.where(y_train == 0)[0]\n",
    "I_1 = np.where(y_train == 1)[0]\n",
    "I_2 = np.where(y_train == 2)[0]\n",
    "\n",
    "# log p(y=c)\n",
    "y_train_0 = np.array([x for x in y_train if x == 0])\n",
    "y_train_1 = np.array([x for x in y_train if x == 1])\n",
    "y_train_2 = np.array([x for x in y_train if x == 2])\n",
    "\n",
    "log_p_train_0 = np.log(y_train_0.size / y_train.size)\n",
    "log_p_train_1 = np.log(y_train_1.size / y_train.size)\n",
    "log_p_train_2 = np.log(y_train_2.size / y_train.size)\n",
    "\n",
    "# p(xd = xdtest | y = c)\n",
    "counts = {}\n",
    "num_words = vectorizer.get_feature_names_out().size\n",
    "train_size = y_train.size\n",
    "for w in range(num_words):\n",
    "    counts[w, 0] = 0\n",
    "    counts[w, 1] = 0\n",
    "    counts[w, 2] = 0\n",
    "for w in range(num_words):\n",
    "    for i in range(train_size):\n",
    "        if D[i,w] == 1:\n",
    "            counts[w, y_train[i]] += 1\n",
    "\n",
    "# log p(y=c)+sum(log(p(xd = xdtest | y = c)))\n",
    "for w in range(num_words):\n",
    "    if D[0, w] == 1:\n",
    "        log_p_train_0 += np.log((counts[w, 0] + alpha) / (I_0.size + alpha * num_words))\n",
    "        log_p_train_1 += np.log((counts[w, 1] + alpha) / (I_1.size + alpha * num_words))\n",
    "        log_p_train_2 += np.log((counts[w, 2] + alpha) / (I_2.size + alpha * num_words))\n",
    "\n",
    "log_p_train_0, log_p_train_1, log_p_train_2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
