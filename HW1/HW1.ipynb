{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1\n",
    "Diego Rivera Garrido(1674196), d.rivera.garrido@student.tue.nl\n",
    "\n",
    "Iliyan vasilev Teofilov(1671952), i.v.teofilov@student.tue.nl\n",
    "\n",
    "Nicolas Martinez van der Looven(2064839), n.martinez.van.der.looven@student.tue.nl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we implemented python functions forthe sigmoide, f and ∇f."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoide(x):\n",
    "    return np.exp(x)/(1+np.exp(x))\n",
    "\n",
    "def f(x):\n",
    "    w,b = x[0],x[1]\n",
    "    #return -np.log(sigmoide(w+b))-np.log(sigmoide(1.5*w+b))-np.log(sigmoide(-2*w-b))\n",
    "    return -0.5*w -b + np.log(1 + np.exp(w+b)) + np.log(1 + np.exp(1.5*w+b))+ np.log(1 + np.exp(-2*w-b))\n",
    "\n",
    "def grad_f(x):\n",
    "    w,b = x[0],x[1]\n",
    "    return np.array([-0.5 + sigmoide(w+b) + 1.5*sigmoide(1.5*w+b) - 2*sigmoide(-2*w-b),\n",
    "            -1 + sigmoide(w+b) + sigmoide(1.5*w+b) - sigmoide(-2*w-b)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we implemented the gradient descent algorithm given starting point w₀,b₀, f, ∇f and the stepsize function η(t). The result is the last iteration function value f(x) and the lowest (best) function value achieved throughout all iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f, grad_f, eta, w_0, b_0, max_iter=100):\n",
    "    x = np.array([w_0,b_0])\n",
    "    fmin = np.inf\n",
    "    for t in range(0,max_iter):\n",
    "        x = x - eta(t)*grad_f(x)\n",
    "        fx = f(x)\n",
    "        if (fx < fmin): \n",
    "            fmin = fx\n",
    "    return fx,fmin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.a\n",
    "Run algorithm with  constant stepsize strategy, η=0.2 constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eta_const(t,c=0.2):\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0932277009188751, 1.0932277009188751)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_descent(f,grad_f,eta_const,1,1,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.b\n",
    "Run algorithm with decreasing step-size strategy, η(t)=0.2/√(t+1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eta_sqrt(t,c=0.2):\n",
    "    return c/(np.sqrt(t+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.6965683333378159, 1.6965683333378159)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_descent(f,grad_f,eta_sqrt,1,1,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.c\n",
    "Run algorithm with multi-step step-size strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eta_multistep(t, milestones=[20,50,80], c=0.2, eta_init=0.2):\n",
    "    factor = len(milestones)\n",
    "    for i in range(len(milestones)):\n",
    "        if t < milestones[i]:\n",
    "            factor = i\n",
    "            break\n",
    "    return eta_init * c**factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.5983060979559247, 1.5983060979559247)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_descent(f,grad_f,eta_multistep,1,1,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Coordinate Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we implemented a function for evaluating f(x)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return (1/2) *(x[0] ** 4) - x[0] * x[1] + (x[1] ** 2) + x[1] * x[2] + (x[2] ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.a\n",
    "To compute argmin_xi f(x), we will compute the partial derivative of f wrt xi and equal it to zero to check the stationary points, in other words, we solve for xi in df(x)/dxi=0. Next, we find which stationary point evaluated on the second derivative wrt xi yields a positive value to find the minimum.\n",
    "\n",
    "For x₁, we solve df/dx₁ = 2x₁^3 - x₂ = 0 and d²f/dx₁² = 6x₁^2 > 0 -> x₁ = (x₂/2)^⅓ \n",
    "\n",
    "For x₂, we solve df/dx₂ = -x₁ + 2x₂ + x₃ = 0 and d²f/dx₂² = 2 > 0 -> x₂ = (x₁ - x₃)/2\n",
    "\n",
    "For x₃, we solve df/dx₃ = x₂ + 2x₃ = 0 and d²f/dx₃² = 2 > 0 -> x₃ = -x₂/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmin(x): # mind the index shift\n",
    "    # x₁ = (x₂/2)^⅓ \n",
    "    argminx0 = pow(x[1]/2, 1/3).real # truncate to real part\n",
    "    # x₂ = (x₁ - x₃)/2\n",
    "    argminx1 = (x[0]-x[2])/2\n",
    "    # x₃ = -x₂/2\n",
    "    argminx2 =  -x[1]/2\n",
    "    return (argminx0,argminx1,argminx2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 0: 1.1447142425533319\n",
      "\n",
      "i = 1: -1.0\n",
      "\n",
      "i = 2: -1.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_0=[2,3,4]\n",
    "for i in range(0,3):\n",
    "    print(\"i = \" + str(i) + \": \" + str(argmin(x_0)[i]))\n",
    "    print()\n",
    "\n",
    "\n",
    "## quick test\n",
    "for t in range(-100, 100):\n",
    "    if ( f( [t, x_0[1], x_0[2]] ) < f( [argmin(x_0)[0], x_0[1], x_0[2]] ) ):\n",
    "        print(\"not min.\")\n",
    "    if ( f( [x_0[0], t, x_0[2]] ) < f( [x_0[0], argmin(x_0)[1], x_0[2]] ) ):\n",
    "        print(\"not min.\")\n",
    "    if ( f( [x_0[0], x_0[1], t] ) < f( [x_0[0], x_0[1], argmin(x_0)[2]] ) ):\n",
    "        print(\"not min.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.b\n",
    "\n",
    "Next, we implement the coordinate descent algorithm given f, the array of argmin and the starting point x0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def coordinate_descent(f, argmin, x_0, max_iter=100):\n",
    "\n",
    "    print(\"t =\", 0, \":\")\n",
    "    print(\"x_t =\", x_0)\n",
    "    print(\"f(x_t) =\", f(x_0))\n",
    "    print()\n",
    "\n",
    "    x_t = x_0\n",
    "    for t in range(1, max_iter+1):\n",
    "        # use x_t to save values of argmin we find\n",
    "        print(\"t =\", t, \":\")\n",
    "        for i in range(0,3):\n",
    "            x_t[i] = argmin(x_t)[i] \n",
    "        print(\"x_t =\", x_t)\n",
    "        print(\"f(x_t) =\", f(x_t))\n",
    "        print()\n",
    "    return x_t, f(x_t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t = 0 :\n",
      "x_t = [1, 20, 5]\n",
      "f(x_t) = 505.5\n",
      "\n",
      "t = 1 :\n",
      "x_t = [2.154434690031884, -1.422782654984058, 0.711391327492029]\n",
      "f(x_t) = 15.355698620925356\n",
      "\n",
      "t = 2 :\n",
      "x_t = [0.4463472463394358, -0.13252204057629663, 0.06626102028814831]\n",
      "f(x_t) = 0.09216788882538113\n",
      "\n",
      "t = 3 :\n",
      "x_t = [0.20232802580270529, 0.06803350275727849, -0.03401675137863924]\n",
      "f(x_t) = -0.00945576233934372\n",
      "\n",
      "t = 4 :\n",
      "x_t = [0.3240143752787107, 0.17901556332867496, -0.08950778166433748]\n",
      "f(x_t) = -0.02845772875821445\n",
      "\n",
      "t = 5 :\n",
      "x_t = [0.4473220068245364, 0.26841489424443693, -0.13420744712221847]\n",
      "f(x_t) = -0.046013572305576095\n",
      "\n",
      "t = 6 :\n",
      "x_t = [0.5119869268540793, 0.3230971869881489, -0.16154859349407444]\n",
      "f(x_t) = -0.05277146245500104\n",
      "\n",
      "t = 7 :\n",
      "x_t = [0.5446293734587898, 0.35308898347643214, -0.17654449173821607]\n",
      "f(x_t) = -0.05480670452774185\n",
      "\n",
      "t = 8 :\n",
      "x_t = [0.5609851830337869, 0.36876483738600147, -0.18438241869300073]\n",
      "f(x_t) = -0.055361558822482594\n",
      "\n",
      "t = 9 :\n",
      "x_t = [0.5691671623915749, 0.37677479054228785, -0.18838739527114393]\n",
      "f(x_t) = -0.05550619728130006\n",
      "\n",
      "t = 10 :\n",
      "x_t = [0.5732586463134279, 0.3808230207922859, -0.19041151039614296]\n",
      "f(x_t) = -0.05554310791253051\n",
      "\n",
      "t = 11 :\n",
      "x_t = [0.5753044491280961, 0.38285797976211955, -0.19142898988105977]\n",
      "f(x_t) = -0.055552430092642414\n",
      "\n",
      "t = 12 :\n",
      "x_t = [0.5763273580847414, 0.3838781739829006, -0.1919390869914503]\n",
      "f(x_t) = -0.05555477249313431\n",
      "\n",
      "t = 13 :\n",
      "x_t = [0.5768388135031562, 0.38438895024730324, -0.19219447512365162]\n",
      "f(x_t) = -0.05555535957769669\n",
      "\n",
      "t = 14 :\n",
      "x_t = [0.5770945413296524, 0.384644508226652, -0.192322254113326]\n",
      "f(x_t) = -0.055555506534548695\n",
      "\n",
      "t = 15 :\n",
      "x_t = [0.5772224052575478, 0.3847723296854369, -0.19238616484271845]\n",
      "f(x_t) = -0.055555543296985435\n",
      "\n",
      "t = 16 :\n",
      "x_t = [0.5772863372233255, 0.38483625103302194, -0.19241812551651097]\n",
      "f(x_t) = -0.055555552490498164\n",
      "\n",
      "t = 17 :\n",
      "x_t = [0.577318303206443, 0.38486821436147695, -0.19243410718073847]\n",
      "f(x_t) = -0.055555554789239354\n",
      "\n",
      "t = 18 :\n",
      "x_t = [0.5773342861980303, 0.3848841966893844, -0.1924420983446922]\n",
      "f(x_t) = -0.05555555536397002\n",
      "\n",
      "t = 19 :\n",
      "x_t = [0.5773422776938275, 0.38489218801925984, -0.19244609400962992]\n",
      "f(x_t) = -0.05555555550765838\n",
      "\n",
      "t = 20 :\n",
      "x_t = [0.5773462734417266, 0.38489618372567824, -0.19244809186283912]\n",
      "f(x_t) = -0.055555555543581145\n",
      "\n",
      "t = 21 :\n",
      "x_t = [0.5773482713156761, 0.3848981815892576, -0.1924490907946288]\n",
      "f(x_t) = -0.05555555555256193\n",
      "\n",
      "t = 22 :\n",
      "x_t = [0.5773492702526509, 0.38489918052363986, -0.19244959026181993]\n",
      "f(x_t) = -0.055555555554807144\n",
      "\n",
      "t = 23 :\n",
      "x_t = [0.5773497697211384, 0.38489967999147917, -0.19244983999573959]\n",
      "f(x_t) = -0.055555555555368466\n",
      "\n",
      "t = 24 :\n",
      "x_t = [0.5773500194553821, 0.38489992972556086, -0.19244996486278043]\n",
      "f(x_t) = -0.05555555555550879\n",
      "\n",
      "t = 25 :\n",
      "x_t = [0.577350144322504, 0.3849000545926422, -0.1924500272963211]\n",
      "f(x_t) = -0.055555555555543874\n",
      "\n",
      "t = 26 :\n",
      "x_t = [0.5773502067560649, 0.384900117026193, -0.1924500585130965]\n",
      "f(x_t) = -0.05555555555555262\n",
      "\n",
      "t = 27 :\n",
      "x_t = [0.5773502379728453, 0.3849001482429709, -0.19245007412148546]\n",
      "f(x_t) = -0.055555555555554845\n",
      "\n",
      "t = 28 :\n",
      "x_t = [0.5773502535812356, 0.3849001638513605, -0.19245008192568025]\n",
      "f(x_t) = -0.05555555555555535\n",
      "\n",
      "t = 29 :\n",
      "x_t = [0.5773502613854307, 0.38490017165555546, -0.19245008582777773]\n",
      "f(x_t) = -0.05555555555555551\n",
      "\n",
      "t = 30 :\n",
      "x_t = [0.5773502652875282, 0.384900175557653, -0.1924500877788265]\n",
      "f(x_t) = -0.05555555555555553\n",
      "\n",
      "t = 31 :\n",
      "x_t = [0.577350267238577, 0.38490017750870176, -0.19245008875435088]\n",
      "f(x_t) = -0.05555555555555553\n",
      "\n",
      "t = 32 :\n",
      "x_t = [0.5773502682141014, 0.38490017848422614, -0.19245008924211307]\n",
      "f(x_t) = -0.05555555555555552\n",
      "\n",
      "t = 33 :\n",
      "x_t = [0.5773502687018636, 0.3849001789719883, -0.19245008948599415]\n",
      "f(x_t) = -0.05555555555555553\n",
      "\n",
      "t = 34 :\n",
      "x_t = [0.5773502689457447, 0.38490017921586944, -0.19245008960793472]\n",
      "f(x_t) = -0.05555555555555555\n",
      "\n",
      "t = 35 :\n",
      "x_t = [0.5773502690676853, 0.38490017933781, -0.192450089668905]\n",
      "f(x_t) = -0.055555555555555546\n",
      "\n",
      "t = 36 :\n",
      "x_t = [0.5773502691286555, 0.38490017939878024, -0.19245008969939012]\n",
      "f(x_t) = -0.055555555555555546\n",
      "\n",
      "t = 37 :\n",
      "x_t = [0.5773502691591407, 0.3849001794292654, -0.1924500897146327]\n",
      "f(x_t) = -0.05555555555555556\n",
      "\n",
      "t = 38 :\n",
      "x_t = [0.5773502691743833, 0.384900179444508, -0.192450089722254]\n",
      "f(x_t) = -0.05555555555555556\n",
      "\n",
      "t = 39 :\n",
      "x_t = [0.5773502691820045, 0.38490017945212923, -0.19245008972606462]\n",
      "f(x_t) = -0.05555555555555558\n",
      "\n",
      "t = 40 :\n",
      "x_t = [0.5773502691858151, 0.38490017945593985, -0.19245008972796993]\n",
      "f(x_t) = -0.05555555555555555\n",
      "\n",
      "t = 41 :\n",
      "x_t = [0.5773502691877205, 0.3849001794578452, -0.1924500897289226]\n",
      "f(x_t) = -0.05555555555555553\n",
      "\n",
      "t = 42 :\n",
      "x_t = [0.5773502691886732, 0.3849001794587979, -0.19245008972939895]\n",
      "f(x_t) = -0.05555555555555555\n",
      "\n",
      "t = 43 :\n",
      "x_t = [0.5773502691891494, 0.3849001794592742, -0.1924500897296371]\n",
      "f(x_t) = -0.05555555555555556\n",
      "\n",
      "t = 44 :\n",
      "x_t = [0.5773502691893876, 0.3849001794595123, -0.19245008972975616]\n",
      "f(x_t) = -0.055555555555555566\n",
      "\n",
      "t = 45 :\n",
      "x_t = [0.5773502691895067, 0.38490017945963145, -0.19245008972981573]\n",
      "f(x_t) = -0.055555555555555566\n",
      "\n",
      "t = 46 :\n",
      "x_t = [0.5773502691895662, 0.38490017945969096, -0.19245008972984548]\n",
      "f(x_t) = -0.05555555555555553\n",
      "\n",
      "t = 47 :\n",
      "x_t = [0.577350269189596, 0.3849001794597207, -0.19245008972986036]\n",
      "f(x_t) = -0.055555555555555566\n",
      "\n",
      "t = 48 :\n",
      "x_t = [0.5773502691896109, 0.3849001794597356, -0.1924500897298678]\n",
      "f(x_t) = -0.055555555555555546\n",
      "\n",
      "t = 49 :\n",
      "x_t = [0.5773502691896183, 0.38490017945974303, -0.19245008972987152]\n",
      "f(x_t) = -0.05555555555555556\n",
      "\n",
      "t = 50 :\n",
      "x_t = [0.5773502691896221, 0.3849001794597468, -0.1924500897298734]\n",
      "f(x_t) = -0.05555555555555554\n",
      "\n",
      "t = 51 :\n",
      "x_t = [0.577350269189624, 0.3849001794597487, -0.19245008972987435]\n",
      "f(x_t) = -0.055555555555555566\n",
      "\n",
      "t = 52 :\n",
      "x_t = [0.5773502691896248, 0.3849001794597496, -0.1924500897298748]\n",
      "f(x_t) = -0.055555555555555546\n",
      "\n",
      "t = 53 :\n",
      "x_t = [0.5773502691896253, 0.38490017945975, -0.192450089729875]\n",
      "f(x_t) = -0.05555555555555557\n",
      "\n",
      "t = 54 :\n",
      "x_t = [0.5773502691896255, 0.38490017945975025, -0.19245008972987512]\n",
      "f(x_t) = -0.05555555555555556\n",
      "\n",
      "t = 55 :\n",
      "x_t = [0.5773502691896256, 0.38490017945975036, -0.19245008972987518]\n",
      "f(x_t) = -0.05555555555555555\n",
      "\n",
      "t = 56 :\n",
      "x_t = [0.5773502691896257, 0.38490017945975047, -0.19245008972987523]\n",
      "f(x_t) = -0.055555555555555546\n",
      "\n",
      "t = 57 :\n",
      "x_t = [0.5773502691896257, 0.38490017945975047, -0.19245008972987523]\n",
      "f(x_t) = -0.055555555555555546\n",
      "\n",
      "t = 58 :\n",
      "x_t = [0.5773502691896257, 0.38490017945975047, -0.19245008972987523]\n",
      "f(x_t) = -0.055555555555555546\n",
      "\n",
      "t = 59 :\n",
      "x_t = [0.5773502691896257, 0.38490017945975047, -0.19245008972987523]\n",
      "f(x_t) = -0.055555555555555546\n",
      "\n",
      "t = 60 :\n",
      "x_t = [0.5773502691896257, 0.38490017945975047, -0.19245008972987523]\n",
      "f(x_t) = -0.055555555555555546\n",
      "\n",
      "t = 61 :\n",
      "x_t = [0.5773502691896257, 0.38490017945975047, -0.19245008972987523]\n",
      "f(x_t) = -0.055555555555555546\n",
      "\n",
      "t = 62 :\n",
      "x_t = [0.5773502691896257, 0.38490017945975047, -0.19245008972987523]\n",
      "f(x_t) = -0.055555555555555546\n",
      "\n",
      "t = 63 :\n",
      "x_t = [0.5773502691896257, 0.38490017945975047, -0.19245008972987523]\n",
      "f(x_t) = -0.055555555555555546\n",
      "\n",
      "t = 64 :\n",
      "x_t = [0.5773502691896257, 0.38490017945975047, -0.19245008972987523]\n",
      "f(x_t) = -0.055555555555555546\n",
      "\n",
      "t = 65 :\n",
      "x_t = [0.5773502691896257, 0.38490017945975047, -0.19245008972987523]\n",
      "f(x_t) = -0.055555555555555546\n",
      "\n",
      "t = 66 :\n",
      "x_t = [0.5773502691896257, 0.38490017945975047, -0.19245008972987523]\n",
      "f(x_t) = -0.055555555555555546\n",
      "\n",
      "t = 67 :\n",
      "x_t = [0.5773502691896257, 0.38490017945975047, -0.19245008972987523]\n",
      "f(x_t) = -0.055555555555555546\n",
      "\n",
      "t = 68 :\n",
      "x_t = [0.5773502691896257, 0.38490017945975047, -0.19245008972987523]\n",
      "f(x_t) = -0.055555555555555546\n",
      "\n",
      "t = 69 :\n",
      "x_t = [0.5773502691896257, 0.38490017945975047, -0.19245008972987523]\n",
      "f(x_t) = -0.055555555555555546\n",
      "\n",
      "t = 70 :\n",
      "x_t = [0.5773502691896257, 0.38490017945975047, -0.19245008972987523]\n",
      "f(x_t) = -0.055555555555555546\n",
      "\n",
      "t = 71 :\n",
      "x_t = [0.5773502691896257, 0.38490017945975047, -0.19245008972987523]\n",
      "f(x_t) = -0.055555555555555546\n",
      "\n",
      "t = 72 :\n",
      "x_t = [0.5773502691896257, 0.38490017945975047, -0.19245008972987523]\n",
      "f(x_t) = -0.055555555555555546\n",
      "\n",
      "t = 73 :\n",
      "x_t = [0.5773502691896257, 0.38490017945975047, -0.19245008972987523]\n",
      "f(x_t) = -0.055555555555555546\n",
      "\n",
      "t = 74 :\n",
      "x_t = [0.5773502691896257, 0.38490017945975047, -0.19245008972987523]\n",
      "f(x_t) = -0.055555555555555546\n",
      "\n",
      "t = 75 :\n",
      "x_t = [0.5773502691896257, 0.38490017945975047, -0.19245008972987523]\n",
      "f(x_t) = -0.055555555555555546\n",
      "\n",
      "t = 76 :\n",
      "x_t = [0.5773502691896257, 0.38490017945975047, -0.19245008972987523]\n",
      "f(x_t) = -0.055555555555555546\n",
      "\n",
      "t = 77 :\n",
      "x_t = [0.5773502691896257, 0.38490017945975047, -0.19245008972987523]\n",
      "f(x_t) = -0.055555555555555546\n",
      "\n",
      "t = 78 :\n",
      "x_t = [0.5773502691896257, 0.38490017945975047, -0.19245008972987523]\n",
      "f(x_t) = -0.055555555555555546\n",
      "\n",
      "t = 79 :\n",
      "x_t = [0.5773502691896257, 0.38490017945975047, -0.19245008972987523]\n",
      "f(x_t) = -0.055555555555555546\n",
      "\n",
      "t = 80 :\n",
      "x_t = [0.5773502691896257, 0.38490017945975047, -0.19245008972987523]\n",
      "f(x_t) = -0.055555555555555546\n",
      "\n",
      "t = 81 :\n",
      "x_t = [0.5773502691896257, 0.38490017945975047, -0.19245008972987523]\n",
      "f(x_t) = -0.055555555555555546\n",
      "\n",
      "t = 82 :\n",
      "x_t = [0.5773502691896257, 0.38490017945975047, -0.19245008972987523]\n",
      "f(x_t) = -0.055555555555555546\n",
      "\n",
      "t = 83 :\n",
      "x_t = [0.5773502691896257, 0.38490017945975047, -0.19245008972987523]\n",
      "f(x_t) = -0.055555555555555546\n",
      "\n",
      "t = 84 :\n",
      "x_t = [0.5773502691896257, 0.38490017945975047, -0.19245008972987523]\n",
      "f(x_t) = -0.055555555555555546\n",
      "\n",
      "t = 85 :\n",
      "x_t = [0.5773502691896257, 0.38490017945975047, -0.19245008972987523]\n",
      "f(x_t) = -0.055555555555555546\n",
      "\n",
      "t = 86 :\n",
      "x_t = [0.5773502691896257, 0.38490017945975047, -0.19245008972987523]\n",
      "f(x_t) = -0.055555555555555546\n",
      "\n",
      "t = 87 :\n",
      "x_t = [0.5773502691896257, 0.38490017945975047, -0.19245008972987523]\n",
      "f(x_t) = -0.055555555555555546\n",
      "\n",
      "t = 88 :\n",
      "x_t = [0.5773502691896257, 0.38490017945975047, -0.19245008972987523]\n",
      "f(x_t) = -0.055555555555555546\n",
      "\n",
      "t = 89 :\n",
      "x_t = [0.5773502691896257, 0.38490017945975047, -0.19245008972987523]\n",
      "f(x_t) = -0.055555555555555546\n",
      "\n",
      "t = 90 :\n",
      "x_t = [0.5773502691896257, 0.38490017945975047, -0.19245008972987523]\n",
      "f(x_t) = -0.055555555555555546\n",
      "\n",
      "t = 91 :\n",
      "x_t = [0.5773502691896257, 0.38490017945975047, -0.19245008972987523]\n",
      "f(x_t) = -0.055555555555555546\n",
      "\n",
      "t = 92 :\n",
      "x_t = [0.5773502691896257, 0.38490017945975047, -0.19245008972987523]\n",
      "f(x_t) = -0.055555555555555546\n",
      "\n",
      "t = 93 :\n",
      "x_t = [0.5773502691896257, 0.38490017945975047, -0.19245008972987523]\n",
      "f(x_t) = -0.055555555555555546\n",
      "\n",
      "t = 94 :\n",
      "x_t = [0.5773502691896257, 0.38490017945975047, -0.19245008972987523]\n",
      "f(x_t) = -0.055555555555555546\n",
      "\n",
      "t = 95 :\n",
      "x_t = [0.5773502691896257, 0.38490017945975047, -0.19245008972987523]\n",
      "f(x_t) = -0.055555555555555546\n",
      "\n",
      "t = 96 :\n",
      "x_t = [0.5773502691896257, 0.38490017945975047, -0.19245008972987523]\n",
      "f(x_t) = -0.055555555555555546\n",
      "\n",
      "t = 97 :\n",
      "x_t = [0.5773502691896257, 0.38490017945975047, -0.19245008972987523]\n",
      "f(x_t) = -0.055555555555555546\n",
      "\n",
      "t = 98 :\n",
      "x_t = [0.5773502691896257, 0.38490017945975047, -0.19245008972987523]\n",
      "f(x_t) = -0.055555555555555546\n",
      "\n",
      "t = 99 :\n",
      "x_t = [0.5773502691896257, 0.38490017945975047, -0.19245008972987523]\n",
      "f(x_t) = -0.055555555555555546\n",
      "\n",
      "t = 100 :\n",
      "x_t = [0.5773502691896257, 0.38490017945975047, -0.19245008972987523]\n",
      "f(x_t) = -0.055555555555555546\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_0=[1,20,5]\n",
    "coordinate_descent(f, argmin, x_0, 100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Regression - polynomial features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.a\n",
    "\n",
    "In this exercise we will be using the California housing dataset from sklearn.\n",
    "\n",
    "We will compute the desing matrix for this dataset using a polynomial with degree 2, using the function PolynomialFeatures from sklearn.preprocessing. Because the data given has values in different big ranges, this function cannot compute the values for the matrix properly. To solve this what we do is standardize the data using the function StandardScaler from sklearn.preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import scipy as sp\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import PolynomialFeatures,StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20640 8\n"
     ]
    }
   ],
   "source": [
    "# Creating the data matrix\n",
    "california = fetch_california_housing()\n",
    "D = california.data\n",
    "y = california.target\n",
    "n,d = D.shape\n",
    "print(n,d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1' 'MedInc' 'HouseAge' 'AveRooms' 'AveBedrms' 'Population' 'AveOccup'\n",
      " 'Latitude' 'Longitude' 'MedInc^2' 'MedInc HouseAge' 'MedInc AveRooms'\n",
      " 'MedInc AveBedrms' 'MedInc Population' 'MedInc AveOccup'\n",
      " 'MedInc Latitude' 'MedInc Longitude' 'HouseAge^2' 'HouseAge AveRooms'\n",
      " 'HouseAge AveBedrms' 'HouseAge Population' 'HouseAge AveOccup'\n",
      " 'HouseAge Latitude' 'HouseAge Longitude' 'AveRooms^2'\n",
      " 'AveRooms AveBedrms' 'AveRooms Population' 'AveRooms AveOccup'\n",
      " 'AveRooms Latitude' 'AveRooms Longitude' 'AveBedrms^2'\n",
      " 'AveBedrms Population' 'AveBedrms AveOccup' 'AveBedrms Latitude'\n",
      " 'AveBedrms Longitude' 'Population^2' 'Population AveOccup'\n",
      " 'Population Latitude' 'Population Longitude' 'AveOccup^2'\n",
      " 'AveOccup Latitude' 'AveOccup Longitude' 'Latitude^2'\n",
      " 'Latitude Longitude' 'Longitude^2']\n"
     ]
    }
   ],
   "source": [
    "# Creating a design matrix with polynomial standardized features\n",
    "aff = PolynomialFeatures(2,include_bias=True)\n",
    "scaler = StandardScaler()\n",
    "X = aff.fit_transform(scaler.fit_transform(D))\n",
    "features = aff.get_feature_names_out(california.feature_names)\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will compute the regression model minimizing the RSS for the polynomial\n",
    "design matrix. To minimize RSS, we solve the system of equations given by ∇(RSS)=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.95724199,  0.92243689,  0.1322746 , -0.39583612,  0.56212582,\n",
       "        0.03851036, -1.68052409, -1.27092793, -1.16447981, -0.11299565,\n",
       "        0.04446976,  0.20353088, -0.16758436,  0.12323738, -0.05110633,\n",
       "       -0.61882733, -0.54901295,  0.03353488, -0.03930124,  0.06328855,\n",
       "        0.04017423, -0.23751839, -0.27066427, -0.25213086,  0.04449819,\n",
       "       -0.08079146, -0.19118335,  0.69090139,  0.46243774,  0.40789392,\n",
       "        0.03597354,  0.2759851 , -0.42913941, -0.44237578, -0.38372053,\n",
       "        0.00351097,  0.22514157,  0.05551793,  0.03325349,  0.00940107,\n",
       "        0.47326301,  0.33794513,  0.28233172,  0.46261665,  0.16018745])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "beta = sp.linalg.solve(X.T@X,X.T@y)\n",
    "beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.92243689]\n",
      "[-0.16758436]\n",
      "[0.06328855]\n"
     ]
    }
   ],
   "source": [
    "# Fetch beta values for given features\n",
    "print(beta[np.where(features == \"MedInc\")])\n",
    "print(beta[np.where(features == \"MedInc AveBedrms\")])\n",
    "print(beta[np.where(features == \"HouseAge AveBedrms\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.b\n",
    "\n",
    "In this subexercise we will repeat what we did in 3.a, in this case using ridge regression with the following objective: \\\n",
    "min f(β) = min 1/n*‖y-Xβ‖²+λ‖β‖²"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.45624916,  0.71957545,  0.14820683, -0.14838337,  0.00862983,\n",
       "       -0.00186671, -0.17832345, -0.47130572, -0.33996516, -0.02359837,\n",
       "        0.05021859,  0.12503662, -0.09998862,  0.05668295, -0.03998195,\n",
       "       -0.06939659, -0.05699217,  0.15959297, -0.0215094 ,  0.0092316 ,\n",
       "        0.01565444, -0.0450344 , -0.03672704, -0.00781034,  0.06979771,\n",
       "       -0.11923069, -0.09952622,  0.05196717,  0.06136376,  0.00995201,\n",
       "        0.04975143,  0.01407802,  0.00709968, -0.00351464,  0.03412183,\n",
       "        0.00363   ,  0.02554756,  0.03123771,  0.00882566,  0.0015518 ,\n",
       "        0.03225161, -0.01687138,  0.04437523, -0.03313319,  0.17547435])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Minimize Objective function f(β)=1/n*‖y-Xβ‖²+λ‖β‖² by solving the system of equations \n",
    "# given by ∇(f(β))=0 for λ=0.1\n",
    "p = X.shape[1]\n",
    "beta_b = sp.linalg.solve(X.T@X+n*0.1*np.eye(p),X.T@y)\n",
    "beta_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.71957545]\n",
      "[-0.09998862]\n",
      "[0.0092316]\n"
     ]
    }
   ],
   "source": [
    "# Fetch beta values for given features\n",
    "print(beta_b[np.where(features == \"MedInc\")])\n",
    "print(beta_b[np.where(features == \"MedInc AveBedrms\")])\n",
    "print(beta_b[np.where(features == \"HouseAge AveBedrms\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias-var trade \n",
    "\n",
    "In this exercise we will compute the bias² and variance of the given regression models: \n",
    "<ul>\n",
    "<li> True regression function: f*(x)=tan(πx)\n",
    "    <li> Fitted three regression models on the i.i.d. data samples D1,D2,D3 obtaining:\n",
    "​    <ul>\n",
    "        <li> f₁(x) = x + 0.2\n",
    "        <li> f₂(x) = 3x + 0.3\n",
    "        <li> f₃(x) = 5x + 0.1\n",
    "    <ul>    \n",
    "<ul>\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define functions to evaluate f*, f1, f2 and f3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fstar(x): return np.tan(np.pi*x)\n",
    "def fd1(x): return x + 0.2\n",
    "def fd2(x): return 3*x + 0.3\n",
    "def fd3(x): return 5*x + 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets plot the function to have and idea of how the regression functions fit the true function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fd656ee2fd0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "x = np.linspace(-0.4,0.4)\n",
    "\n",
    "plt.plot(x,fstar(x), label='f*')\n",
    "plt.plot(x,fd1(x), label='f1')\n",
    "plt.plot(x,fd2(x), label='f2')\n",
    "plt.plot(x,fd3(x), label='f3')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, lets compute for x₀=0 the bias² and the variance of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.039999999999999994\n",
      "0.006666666666666665\n"
     ]
    }
   ],
   "source": [
    "fdx0 = np.array([fd1(0),fd2(0),fd3(0)])\n",
    "E = np.mean(fdx0)\n",
    "bias = (fstar(0)-E)\n",
    "var = np.mean((E-fdx0)**2)\n",
    "\n",
    "print(bias**2)\n",
    "print(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes - 20news\n",
    "From the 20Newsgroups dataset we fetch the documents belonging to three categories, which we use as classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "categories = ['alt.atheism', 'talk.politics.guns',\n",
    "              'sci.space']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = fetch_20newsgroups(subset='train', categories=categories)\n",
    "test = fetch_20newsgroups(subset='test', categories=categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, the first document in the training data is the following one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: fcrary@ucsu.Colorado.EDU (Frank Crary)\n",
      "Subject: Re: Riddle me this...\n",
      "Nntp-Posting-Host: ucsu.colorado.edu\n",
      "Organization: University of Colorado, Boulder\n",
      "Distribution: usa\n",
      "Lines: 16\n",
      "\n",
      "In article <1r1lp1INN752@mojo.eng.umd.edu> chuck@eng.umd.edu (Chuck Harris - WA3UQV) writes:\n",
      ">>If so, why was CS often employed against tunnels in Vietnam?\n",
      "\n",
      ">CS \"tear-gas\" was used in Vietnam because it makes you wretch so hard that\n",
      ">your stomach comes out thru your throat.  Well, not quite that bad, but\n",
      ">you can't really do much to defend yourself while you are blowing cookies.\n",
      "\n",
      "I think the is BZ gas, not CS or CN. BZ gas exposure results in projectile\n",
      "vomiting, loss of essentially all muscle control, inability to concentrate\n",
      "or think rationally and fatal reactions in a significant fraction of\n",
      "the population. For that reason its use is limited to military\n",
      "applications.\n",
      "\n",
      "                                                          Frank Crary\n",
      "                                                          CU Boulder\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(train.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classes are indicated categorically with indices from zero to two by the target vector. The target names tell us which index belongs to which class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 1, ..., 1, 2, 2])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = train.target\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism', 'sci.space', 'talk.politics.guns']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We represent the documents in a bag of word format. That is, we create a data matrix ``D`` such that ``D[j,i]=1`` if the j-th document contains the i-th feature (word), and ``D[j,i]=0`` otherwise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(stop_words=\"english\", min_df=5,token_pattern=\"[^\\W\\d_]+\", binary=True)\n",
    "D = vectorizer.fit_transform(train.data)\n",
    "D_test = vectorizer.transform(test.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the allocation of feature indices to words by the following array, containing the vocabulary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['aa', 'aario', 'aaron', ..., 'zoology', 'zv', 'ÿ'], dtype=object)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, the word `naive` has the index 4044."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4044])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(vectorizer.get_feature_names_out() == 'naive')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.a\n",
    "First lets compute the class prior probabilities p(y):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2964793082149475, 0.3662754786905497, 0.3372452130945028)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_0 = np.array([x for x in y_train if x == 0])\n",
    "y_train_1 = np.array([x for x in y_train if x == 1])\n",
    "y_train_2 = np.array([x for x in y_train if x == 2])\n",
    "\n",
    "p_train_0 = y_train_0.size / y_train.size\n",
    "p_train_1 = y_train_1.size / y_train.size\n",
    "p_train_2 = y_train_2.size / y_train.size\n",
    "\n",
    "p_train_0, p_train_1, p_train_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.b\n",
    "Lets continue by computing the log-probabilities of the word 'naive' given each class, using Laplace smoothing with α=1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.56448951, -6.38530041, -4.91644811])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = 1e-5\n",
    "# Count number of documents of each class in the training set\n",
    "I_0 = np.where(y_train == 0)[0]\n",
    "I_1 = np.where(y_train == 1)[0]\n",
    "I_2 = np.where(y_train == 2)[0]\n",
    "\n",
    "# Count number of documents per class which includes the word 'naive'\n",
    "class_counts = {0: 0, 1: 0, 2: 0}\n",
    "for i in range(y_train.size):\n",
    "    if D[i, 4044] == 1:\n",
    "        class_counts[y_train[i]] += 1\n",
    "\n",
    "K = vectorizer.get_feature_names_out().size\n",
    "\n",
    "p_train_0 = (class_counts[0] + alpha) / (I_0.size + alpha * K)\n",
    "p_train_1 = (class_counts[1] + alpha) / (I_1.size + alpha * K)\n",
    "p_train_2 = (class_counts[2] + alpha) / (I_2.size + alpha * K)\n",
    "\n",
    "np.log(np.array([p_train_0, p_train_1, p_train_2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.c\n",
    "Finally, we will compute the class-conditioned log-probabilities for each word and class combination for the first document in the training dataset, using the naive Bayes algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-362.54772451864125, -381.4478315382202, -193.47950987244982)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = 1e-5\n",
    "# Count number of documents of each class in the training set\n",
    "I_0 = np.where(y_train == 0)[0]\n",
    "I_1 = np.where(y_train == 1)[0]\n",
    "I_2 = np.where(y_train == 2)[0]\n",
    "\n",
    "# log p(y=c)\n",
    "y_train_0 = np.array([x for x in y_train if x == 0])\n",
    "y_train_1 = np.array([x for x in y_train if x == 1])\n",
    "y_train_2 = np.array([x for x in y_train if x == 2])\n",
    "\n",
    "log_p_train_0 = np.log(y_train_0.size / y_train.size)\n",
    "log_p_train_1 = np.log(y_train_1.size / y_train.size)\n",
    "log_p_train_2 = np.log(y_train_2.size / y_train.size)\n",
    "\n",
    "# p(xd = xdtest | y = c)\n",
    "counts = {}\n",
    "num_words = vectorizer.get_feature_names_out().size\n",
    "train_size = y_train.size\n",
    "for w in range(num_words):\n",
    "    counts[w, 0] = 0\n",
    "    counts[w, 1] = 0\n",
    "    counts[w, 2] = 0\n",
    "for w in range(num_words):\n",
    "    for i in range(train_size):\n",
    "        if D[i,w] == 1:\n",
    "            counts[w, y_train[i]] += 1\n",
    "\n",
    "# log p(y=c)+sum(log(p(xd = xdtest | y = c)))\n",
    "for w in range(num_words):\n",
    "    if D[0, w] == 1:\n",
    "        log_p_train_0 += np.log((counts[w, 0] + alpha) / (I_0.size + alpha * num_words))\n",
    "        log_p_train_1 += np.log((counts[w, 1] + alpha) / (I_1.size + alpha * num_words))\n",
    "        log_p_train_2 += np.log((counts[w, 2] + alpha) / (I_2.size + alpha * num_words))\n",
    "\n",
    "log_p_train_0, log_p_train_1, log_p_train_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree \n",
    "Decision tree on iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "D, y = iris.data, iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6a\n",
    "We find the different probabilities in the root node, and apply the following formula to find the Gini impurity:\n",
    "$$G = 1 - \\sum\\limits_y p(y)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666667\n"
     ]
    }
   ],
   "source": [
    "root_lables = y\n",
    "\n",
    "# Total number of instances in the dataset\n",
    "root_total = len(root_lables)\n",
    "\n",
    "# Calculate number of entries for each class\n",
    "unique_classes, class_counts = np.unique(root_lables, return_counts=True)\n",
    "\n",
    "# Calculate probabilities for each class\n",
    "probabilities = (class_counts / root_total)\n",
    "\n",
    "impurity_root = 1 - np.sum(probabilities**2)\n",
    "print(impurity_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6b\n",
    "We now first divide into the left and right leaf and compute the probabilities and gini impurity for each. \n",
    "The the cost is simply\n",
    "$$\\left( \\frac{N_0}{N} \\text{impurity}(L0) + \\frac{N_1}{N} \\text{impurity}(L1) \\right) - \\text{impurity}(L) $$\n",
    "where L0, L1, L are the left leaf, right leaf and root respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.17476190476190478\n"
     ]
    }
   ],
   "source": [
    "split_cond = 5.84\n",
    "total_instances = len(D)\n",
    "\n",
    "# Split the dataset based on the sepal length\n",
    "left_leaf= np.where(D[:, 0] <= split_cond)\n",
    "right_leaf= np.where(D[:, 0] > split_cond)\n",
    "\n",
    "left_lables = y[left_leaf]\n",
    "right_lables = y[right_leaf]\n",
    "\n",
    "# Calculate number of entries for each class and probabilities\n",
    "left_total = len(left_lables)\n",
    "unique_classes, class_count_left = np.unique(left_lables, return_counts=True)\n",
    "left_probabilities = (class_count_left / len(left_lables))\n",
    "impurity_left = 1 - np.sum(left_probabilities**2)\n",
    "\n",
    "right_total = len(right_lables)\n",
    "unique_classes, class_count_right = np.unique(right_lables, return_counts=True)\n",
    "right_probabilities = (class_count_right / len(right_lables))\n",
    "impurity_right = 1 - np.sum(right_probabilities**2)\n",
    "\n",
    "cost = (left_total / root_total) * impurity_left + (right_total / root_total) * impurity_right - impurity_root\n",
    "print(cost)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
